<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>The Evaluation of Sentence Similarity | 糖果AIGC实验室-LUA</title><meta name="author" content="糖果AIGC实验室 备案:辽ICP备16003836号-5"><meta name="copyright" content="糖果AIGC实验室 备案:辽ICP备16003836号-5"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should adm">
<meta property="og:type" content="article">
<meta property="og:title" content="The Evaluation of Sentence Similarity">
<meta property="og:url" content="https://lua.ren/zl/2016-01-01-771_The%20Evaluation%20of%20Sentence%20Similarity/index.html">
<meta property="og:site_name" content="糖果AIGC实验室-LUA">
<meta property="og:description" content="I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should adm">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2024-04-03T03:47:36.010Z">
<meta property="article:modified_time" content="2024-04-03T03:47:36.010Z">
<meta property="article:author" content="糖果AIGC实验室 备案:辽ICP备16003836号-5">
<meta property="article:tag" content="lua文章">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lua.ren/zl/2016-01-01-771_The%20Evaluation%20of%20Sentence%20Similarity/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?14cc93bf3f08d31c458639d309dde522";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();



</script><script>(function () {
  var meta = document.createElement('meta');
  meta.content = 'no-referrer';
  meta.name = 'referrer';
  document.getElementsByTagName('head')[0].appendChild(meta);
})();</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"OCGZTJTHCQ","apiKey":"4dede33603574dcaed0e9d4641849158","indexName":"jianpan.vip","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'The Evaluation of Sentence Similarity',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-04-03 11:47:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1769</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">195</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">96</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="糖果AIGC实验室-LUA"><span class="site-name">糖果AIGC实验室-LUA</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清單</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">The Evaluation of Sentence Similarity</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-04-03T03:47:36.010Z" title="发表于 2024-04-03 11:47:36">2024-04-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-04-03T03:47:36.010Z" title="更新于 2024-04-03 11:47:36">2024-04-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/topic/">topic</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="The Evaluation of Sentence Similarity"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><p>I am trying to write my first english blog based on two reasons: First, the data set used in this blog is english; Second, I’d like to expand my reach and attract more audiences, although I should admit that nobody cares.</p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>Initially I want to use chinese corpus, but I cannot find a proper one. The data should sound like this one:</p>
<blockquote>
<p>word1    word2    similarity score<br/>阿拉伯人    阿拉伯    7.2<br/>畜产    农业    5.6<br/>垂涎    崇敬    3.4<br/>次序    秩序    4.7<br/>定心丸    药品    4.3<br/>房租    价格    5.2<br/>翡翠    宝石    6.7<br/>高科技    技术    7.5<br/>购入    购买    8.5<br/>观音    菩萨    8.2<br/>归并    合并    7.7</p>
</blockquote>
<p>not like this:</p>
<blockquote>
<p>为何我无法申请开通花呗信用卡收款    支付宝开通信用卡花呗收款不符合条件怎么回事    1<br/>花呗分期付款会影响使用吗    花呗分期有什么影响吗    0<br/>为什么我花呗没有临时额度    花呗没有临时额度怎么可以负    0<br/>能不能开花呗老兄    花呗逾期了还能开通    0<br/>我的怎么开通花呗收钱    这个花呗是个什么啥？我没开通 我怎么有账单    0<br/>蚂蚁借呗可以停掉么    蚂蚁借呗为什么给我关掉了    0<br/>我想把花呗功能关了    我去饭店吃饭，能用花呗支付吗    0<br/>为什么我借呗开通了又关闭了    为什么借呗存在风险    0<br/>支付宝被冻了花呗要怎么还    支付功能冻结了，花呗还不了怎么办    1</p>
</blockquote>
<p>If you can find the dataset where ‘similarity score’ is double, please donot hesitate to <a href="mailto:jiajizhengbuaa@gmail.com" target="_blank" rel="noopener noreferrer">email me.</a></p>
<p>So, the choice has to be enlgish corpus. The dataset used in this experiment are <a href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark" target="_blank" rel="noopener noreferrer">STSbenchmark</a> and SICK data. The SICK data contains 10,000 sentence paris labeled with semantic relatedness and entailment relation.<br/><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1sqxx3vl0j219e07yaam.jpg" alt=""/></p>
<h2 id="Similarity-Methods"><a href="#Similarity-Methods" class="headerlink" title="Similarity Methods"></a>Similarity Methods</h2><h3 id="Baseline"><a href="#Baseline" class="headerlink" title="Baseline"></a>Baseline</h3><p>As the baseline, we just take the embedding of the words in sentence, and compute the average, weighted by frequency of each word.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="params">(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None)</span>:</span></span><br/><span class="line">    <span class="keyword">if</span> doc_freqs <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br/><span class="line">        N = doc_freqs[<span class="string">&#34;NUM_DOCS&#34;</span>]</span><br/><span class="line"></span><br/><span class="line">    sims = []</span><br/><span class="line">    <span class="keyword">for</span> (sent1, sent2) <span class="keyword">in</span> zip(sentences1, sentences2):</span><br/><span class="line"></span><br/><span class="line">        tokens1 = sent1.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent1.tokens</span><br/><span class="line">        tokens2 = sent2.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent2.tokens</span><br/><span class="line"></span><br/><span class="line">        tokens1 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens1 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line">        tokens2 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens2 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line">l</span><br/><span class="line">        <span class="keyword">if</span> len(tokens1) == <span class="number">0</span> <span class="keyword">or</span> len(tokens2) == <span class="number">0</span>:</span><br/><span class="line">            sims.append(<span class="number">0</span>)</span><br/><span class="line">            <span class="keyword">continue</span></span><br/><span class="line"></span><br/><span class="line">        tokfreqs1 = Counter(tokens1)</span><br/><span class="line">        tokfreqs2 = Counter(tokens2)</span><br/><span class="line"></span><br/><span class="line">        weights1 = [tokfreqs1[token] * math.log(N / (doc_freqs.get(token, <span class="number">0</span>) + <span class="number">1</span>))</span><br/><span class="line">                    <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs1] <span class="keyword">if</span> doc_freqs <span class="keyword">else</span> <span class="keyword">None</span></span><br/><span class="line">        weights2 = [tokfreqs2[token] * math.log(N / (doc_freqs.get(token, <span class="number">0</span>) + <span class="number">1</span>))</span><br/><span class="line">                    <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs2] <span class="keyword">if</span> doc_freqs <span class="keyword">else</span> <span class="keyword">None</span></span><br/><span class="line"></span><br/><span class="line">        embedding1 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs1], axis=<span class="number">0</span>, weights=weights1).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br/><span class="line">        embedding2 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokfreqs2], axis=<span class="number">0</span>, weights=weights2).reshape(<span class="number">1</span>, <span class="number">-1</span>)</span><br/><span class="line"></span><br/><span class="line">        sim = cosine_similarity(embedding1, embedding2)[<span class="number">0</span>][<span class="number">0</span>]</span><br/><span class="line">        sims.append(sim)</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">return</span> sims</span><br/></pre></td></tr></tbody></table></figure>
<h3 id="Smooth-Inverse-Frequency"><a href="#Smooth-Inverse-Frequency" class="headerlink" title="Smooth Inverse Frequency"></a>Smooth Inverse Frequency</h3><p>The baseline, like we did before, is very simple and crude of computing sentence embedding. Word frequency cannot reliably reflect its importance to sentence, semantically speaking. Smooth Inverse Frequency (SIF) tries to solve this problem.</p>
<ul>
<li>SIF is very similar to the weighted average we used before, with the difference that it’s weighted by this formular.<br/>$$<br/>operatorname { SIF } ( w ) = frac { a } { ( a + p ( w ) )}<br/>$$<br/>where $a$ is a hyper-parameter (set to 0.001 by default) and $ p(w)$ is the estimated word frequency in the corpus. (这个权重和 TF或者 IDF 都是不相同的)</li>
<li>we need to perform common component removal: subtract from the sentence embedding obtained above the first principal component of the matrix. This corrects for the influence of high-frequency words that have syntactic or dicourse function, such as ‘but’, ‘and’, etc. You can find more information from <a href="https://openreview.net/pdf?id=SyK00v5xx" target="_blank" rel="noopener noreferrer">this paper</a>. 因为这个的输入直接是句子，没有经过分词的处理，所以不免有 but and 这类的词汇出现。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/><span class="line">26</span><br/><span class="line">27</span><br/><span class="line">28</span><br/><span class="line">29</span><br/><span class="line">30</span><br/><span class="line">31</span><br/><span class="line">32</span><br/><span class="line">33</span><br/><span class="line">34</span><br/><span class="line">35</span><br/><span class="line">36</span><br/><span class="line">37</span><br/></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_first_principal_component</span><span class="params">(X)</span>:</span></span><br/><span class="line">    svd = TruncatedSVD(n_components=<span class="number">1</span>, n_iter=<span class="number">7</span>, random_state=<span class="number">0</span>)</span><br/><span class="line">    svd.fit(X)</span><br/><span class="line">    pc = svd.components_</span><br/><span class="line">    XX = X - X.dot(pc.transpose()) * pc</span><br/><span class="line">    <span class="keyword">return</span> XX</span><br/><span class="line"></span><br/><span class="line"></span><br/><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_sif_benchmark</span><span class="params">(sentences1, sentences2, model, freqs={}, use_stoplist=False, a=<span class="number">0.001</span>)</span>:</span></span><br/><span class="line">    total_freq = sum(freqs.values())</span><br/><span class="line"></span><br/><span class="line">    embeddings = []</span><br/><span class="line"></span><br/><span class="line">    </span><br/><span class="line">    <span class="comment"># common component analysis.</span></span><br/><span class="line">    <span class="keyword">for</span> (sent1, sent2) <span class="keyword">in</span> zip(sentences1, sentences2):</span><br/><span class="line">        tokens1 = sent1.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent1.tokens</span><br/><span class="line">        tokens2 = sent2.tokens_without_stop <span class="keyword">if</span> use_stoplist <span class="keyword">else</span> sent2.tokens</span><br/><span class="line"></span><br/><span class="line">        tokens1 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens1 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line">        tokens2 = [token <span class="keyword">for</span> token <span class="keyword">in</span> tokens2 <span class="keyword">if</span> token <span class="keyword">in</span> model]</span><br/><span class="line"></span><br/><span class="line">        weights1 = [a / (a + freqs.get(token, <span class="number">0</span>) / total_freq) <span class="keyword">for</span> token <span class="keyword">in</span> tokens1]</span><br/><span class="line">        weights2 = [a / (a + freqs.get(token, <span class="number">0</span>) / total_freq) <span class="keyword">for</span> token <span class="keyword">in</span> tokens2]</span><br/><span class="line"></span><br/><span class="line">        embedding1 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens1], axis=<span class="number">0</span>, weights=weights1)</span><br/><span class="line">        embedding2 = np.average([model[token] <span class="keyword">for</span> token <span class="keyword">in</span> tokens2], axis=<span class="number">0</span>, weights=weights2)</span><br/><span class="line"></span><br/><span class="line">        embeddings.append(embedding1)</span><br/><span class="line">        embeddings.append(embedding2)</span><br/><span class="line"></span><br/><span class="line">    embeddings = remove_first_principal_component(np.array(embeddings))</span><br/><span class="line">    sims = [cosine_similarity(embeddings[idx * <span class="number">2</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>),</span><br/><span class="line">                              embeddings[idx * <span class="number">2</span> + <span class="number">1</span>].reshape(<span class="number">1</span>, <span class="number">-1</span>))[<span class="number">0</span>][<span class="number">0</span>]</span><br/><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> range(int(len(embeddings) / <span class="number">2</span>))]</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">return</span> sims</span><br/></pre></td></tr></tbody></table></figure>
<h3 id="Google-Sentence-Encoder"><a href="#Google-Sentence-Encoder" class="headerlink" title="Google Sentence Encoder"></a>Google Sentence Encoder</h3><p><a href="https://github.com/facebookresearch/InferSent" target="_blank" rel="noopener noreferrer">InferSent</a> is a pre-trained encoder that produces sentence embedding, which opensourced by Facebook. <a href="https://tfhub.dev/google/universal-sentence-encoder/1" target="_blank" rel="noopener noreferrer">The Google Sentence Encoder</a> is Google’s answer to Facebook’s InferSent. In contrast to InferSent, the Google Sentence Encoder was trained on a combination of unsupervised data and supervised data (SNLI corpus), which tends to give better results.</p>
<p>The codes can be used in <a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true" target="_blank" rel="noopener noreferrer">Google Jupyter Notebook</a></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/><span class="line">15</span><br/><span class="line">16</span><br/><span class="line">17</span><br/><span class="line">18</span><br/><span class="line">19</span><br/><span class="line">20</span><br/><span class="line">21</span><br/><span class="line">22</span><br/><span class="line">23</span><br/><span class="line">24</span><br/><span class="line">25</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_hub <span class="keyword">as</span> hub</span><br/><span class="line"></span><br/><span class="line">tf.logging.set_verbosity(tf.logging.ERROR)</span><br/><span class="line">embed = hub.Module(<span class="string">&#34;https://tfhub.dev/google/universal-sentence-encoder/1&#34;</span>)</span><br/><span class="line"></span><br/><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_gse_benchmark</span><span class="params">(sentences1, sentences2)</span>:</span></span><br/><span class="line">    sts_input1 = tf.placeholder(tf.string, shape=(<span class="keyword">None</span>))</span><br/><span class="line">    sts_input2 = tf.placeholder(tf.string, shape=(<span class="keyword">None</span>))</span><br/><span class="line"></span><br/><span class="line">    sts_encode1 = tf.nn.l2_normalize(embed(sts_input1))</span><br/><span class="line">    sts_encode2 = tf.nn.l2_normalize(embed(sts_input2))</span><br/><span class="line"></span><br/><span class="line">    sim_scores = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=<span class="number">1</span>)</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br/><span class="line">        session.run(tf.global_variables_initializer())</span><br/><span class="line">        session.run(tf.tables_initializer())</span><br/><span class="line"></span><br/><span class="line">        [gse_sims] = session.run(</span><br/><span class="line">            [sim_scores],</span><br/><span class="line">            feed_dict={</span><br/><span class="line">                sts_input1: [sent1.raw <span class="keyword">for</span> sent1 <span class="keyword">in</span> sentences1],</span><br/><span class="line">                sts_input2: [sent2.raw <span class="keyword">for</span> sent2 <span class="keyword">in</span> sentences2]</span><br/><span class="line">            })</span><br/><span class="line">    <span class="keyword">return</span> gse_sims</span><br/></pre></td></tr></tbody></table></figure>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/><span class="line">12</span><br/><span class="line">13</span><br/><span class="line">14</span><br/></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_experiment</span><span class="params">(df, benchmarks)</span>:</span></span><br/><span class="line">    sentences1 = [Sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&#39;sent_1&#39;</span>]]</span><br/><span class="line">    sentences2 = [Sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> df[<span class="string">&#39;sent_2&#39;</span>]]</span><br/><span class="line"></span><br/><span class="line">    pearson_cors, spearman_cors = [], []</span><br/><span class="line">    <span class="keyword">for</span> label, method <span class="keyword">in</span> benchmarks:</span><br/><span class="line">        sims = method(sentences1, sentences2)</span><br/><span class="line">        pearson_correlation = scipy.stats.pearsonr(sims, df[<span class="string">&#39;sim&#39;</span>])[<span class="number">0</span>]</span><br/><span class="line">        print(label, pearson_correlation)</span><br/><span class="line">        pearson_cors.append(pearson_correlation)</span><br/><span class="line">        spearman_correlation = scipy.stats.spearmanr(sims, df[<span class="string">&#39;sim&#39;</span>])[<span class="number">0</span>]</span><br/><span class="line">        spearman_cors.append(spearman_correlation)</span><br/><span class="line"></span><br/><span class="line">    <span class="keyword">return</span> pearson_cors, spearman_cors</span><br/></pre></td></tr></tbody></table></figure>
<p>Helper function:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/><span class="line">4</span><br/><span class="line">5</span><br/><span class="line">6</span><br/><span class="line">7</span><br/><span class="line">8</span><br/><span class="line">9</span><br/><span class="line">10</span><br/><span class="line">11</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools <span class="keyword">as</span> ft</span><br/><span class="line"></span><br/><span class="line">benchmarks = [</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">False</span>)),</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE-STOP&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">True</span>)),</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE-TFIDF&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">False</span>, doc_freqs=doc_frequencies)),</span><br/><span class="line">    (<span class="string">&#34;AVG-GLOVE-TFIDF-STOP&#34;</span>, ft.partial(run_avg_benchmark, model=glove, use_stoplist=<span class="keyword">True</span>, doc_freqs=doc_frequencies)),</span><br/><span class="line">    (<span class="string">&#34;SIF-W2V&#34;</span>, ft.partial(run_sif_benchmark, freqs=frequencies, model=word2vec, use_stoplist=<span class="keyword">False</span>)),</span><br/><span class="line">    (<span class="string">&#34;SIF-GLOVE&#34;</span>, ft.partial(run_sif_benchmark, freqs=frequencies, model=glove, use_stoplist=<span class="keyword">False</span>)),</span><br/><span class="line"></span><br/><span class="line">]</span><br/></pre></td></tr></tbody></table></figure>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br/><span class="line">2</span><br/><span class="line">3</span><br/></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br/><span class="line">plt.rcParams[<span class="string">&#39;figure.figsize&#39;</span>] = (<span class="number">20</span>,<span class="number">13</span>)</span><br/><span class="line">spearman[[<span class="string">&#39;AVG-GLOVE&#39;</span>, <span class="string">&#39;AVG-GLOVE-STOP&#39;</span>,<span class="string">&#39;AVG-GLOVE-TFIDF&#39;</span>, <span class="string">&#39;AVG-GLOVE-TFIDF-STOP&#39;</span>,<span class="string">&#39;GSE&#39;</span>]].plot(kind=<span class="string">&#34;bar&#34;</span>).legend(loc=<span class="string">&#34;lower left&#34;</span>)</span><br/></pre></td></tr></tbody></table></figure>
<p><strong>Take Off</strong></p>
<ul>
<li>Smooth Inverse Frequency methods are better than baseline, no matter with word2vec or Glove embeddings.</li>
<li>Google Sentence Encoder has the similar performance as Smooth Inverse Frequency.</li>
<li>Using tf-idf weights does not help and using a stoplist looks like a reasonable choice.</li>
</ul>
<p>Pearson Correlation<br/><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""/><br/>Spearman Correlation<br/><img src="http://ww1.sinaimg.cn/large/e9a223b5ly1g1swpqj9d0j20ps0h5t8z.jpg" alt=""/></p>
<p>Full codes can be found in <a href="https://github.com/jijeng/sentence-similarity" target="_blank" rel="noopener noreferrer">here</a>.</p>
<h2 id="复习笔记"><a href="#复习笔记" class="headerlink" title="复习笔记"></a>复习笔记</h2><ol>
<li>TF-IDF 和 SIF三者的差别</li>
</ol>
<p>SIF的计算公式：<br/>$$<br/>operatorname { SIF } ( w ) = frac { a } { ( a + p ( w ) )}<br/>$$<br/>$a$ 是超参数，一般设置为0.001，保证…;  $p(w)$ 是word 在预料中出现的频数。</p>
<p>TF 的计算公式：</p>
<p>$$ 词频(TF) = 某个词在文章中出现的次数( 频数) $$</p>
<p>可以进一步标准化（减少文章长度的影响）</p>
<p>$$ 词频( TF) = frac{某次在文中出现的次数}{文章的总词语数} $$</p>
<p>$$ 逆文档频率 (IDF) = log(frac{语料中的文档总数}{ 包含该词的文档数 +1}) $$ </p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://lua.ren">糖果AIGC实验室 备案:辽ICP备16003836号-5</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lua.ren/zl/2016-01-01-771_The%20Evaluation%20of%20Sentence%20Similarity/">https://lua.ren/zl/2016-01-01-771_The%20Evaluation%20of%20Sentence%20Similarity/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="null" target="_blank">null</a> 许可协议。转载请注明来自 <a href="https://lua.ren" target="_blank">糖果AIGC实验室-LUA</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/lua%E6%96%87%E7%AB%A0/">lua文章</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/zl/2016-01-01-773_%E7%94%A8%20lua%20%E6%A8%A1%E6%8B%9F%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%20%C2%B7%20dcf's%20blog/" title="用 lua 模拟面向对象 · dcf's blog"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">用 lua 模拟面向对象 · dcf's blog</div></div></a></div><div class="next-post pull-right"><a href="/zl/2016-01-01-772_%E5%9C%A8Lua%E4%B8%AD%E8%B0%83%E7%94%A8C%E5%87%BD%E6%95%B0/" title="在Lua中调用C函数"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">在Lua中调用C函数</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/zl/2016-01-01-2_AutoToolboxPopluate/" title="AutoToolboxPopluate"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-03</div><div class="title">AutoToolboxPopluate</div></div></a></div><div><a href="/zl/2016-01-01-1001_Lua%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/" title="Lua正则表达式"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-03</div><div class="title">Lua正则表达式</div></div></a></div><div><a href="/zl/2016-01-01-1003_lua%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/" title="lua快速入门"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-03</div><div class="title">lua快速入门</div></div></a></div><div><a href="/zl/2016-01-01-1004_C#%20%E4%BD%BF%E7%94%A8%20Lua%20%E5%8F%96%E5%BE%97%20Redis%20%E8%87%AA%E8%A8%82%E8%A4%87%E9%9B%9C%E5%9E%8B%E5%88%A5/" title="C# 使用 Lua 取得 Redis 自訂複雜型別"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-03</div><div class="title">C# 使用 Lua 取得 Redis 自訂複雜型別</div></div></a></div><div><a href="/zl/2016-01-01-1005_Lua%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/" title="Lua语言学习（二）"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-03</div><div class="title">Lua语言学习（二）</div></div></a></div><div><a href="/zl/2016-01-01-1002_Lua%20%E5%AD%A6%E4%B9%A0%20chapter7%20/" title="Lua 学习 chapter7"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-03</div><div class="title">Lua 学习 chapter7</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">糖果AIGC实验室 备案:辽ICP备16003836号-5</div><div class="author-info__description">AIGC</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">1769</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">195</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">96</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">公众号：糖果的实验室 <img src="https://gitee.com/shengnoah/picture/raw/master/20231027182204.png"></img></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Data"><span class="toc-number">1.</span> <span class="toc-text">Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Similarity-Methods"><span class="toc-number">2.</span> <span class="toc-text">Similarity Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Baseline"><span class="toc-number">2.1.</span> <span class="toc-text">Baseline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Smooth-Inverse-Frequency"><span class="toc-number">2.2.</span> <span class="toc-text">Smooth Inverse Frequency</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Google-Sentence-Encoder"><span class="toc-number">2.3.</span> <span class="toc-text">Google Sentence Encoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experiments"><span class="toc-number">3.</span> <span class="toc-text">Experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Results"><span class="toc-number">4.</span> <span class="toc-text">Results</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0"><span class="toc-number">5.</span> <span class="toc-text">复习笔记</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/zl/2016-01-01-99_%E6%B7%B1%E5%85%A5%20Lua%20Garbage%20Collector(%E4%B8%89)/" title="深入 Lua Garbage Collector(三)"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深入 Lua Garbage Collector(三)"/></a><div class="content"><a class="title" href="/zl/2016-01-01-99_%E6%B7%B1%E5%85%A5%20Lua%20Garbage%20Collector(%E4%B8%89)/" title="深入 Lua Garbage Collector(三)">深入 Lua Garbage Collector(三)</a><time datetime="2024-04-03T03:47:36.244Z" title="发表于 2024-04-03 11:47:36">2024-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/zl/2016-01-01-9_Evaluating%20Polynomials/" title="Evaluating Polynomials"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Evaluating Polynomials"/></a><div class="content"><a class="title" href="/zl/2016-01-01-9_Evaluating%20Polynomials/" title="Evaluating Polynomials">Evaluating Polynomials</a><time datetime="2024-04-03T03:47:36.244Z" title="发表于 2024-04-03 11:47:36">2024-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/zl/2016-01-01-999_%E6%B7%B1%E5%85%A5Lua%20/" title="深入Lua"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="深入Lua"/></a><div class="content"><a class="title" href="/zl/2016-01-01-999_%E6%B7%B1%E5%85%A5Lua%20/" title="深入Lua">深入Lua</a><time datetime="2024-04-03T03:47:36.244Z" title="发表于 2024-04-03 11:47:36">2024-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/zl/2016-01-01-996_evaluation%20and%20selection%20%C2%B7%20ngc7293's%20blog/" title="evaluation and selection · ngc7293's blog"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="evaluation and selection · ngc7293's blog"/></a><div class="content"><a class="title" href="/zl/2016-01-01-996_evaluation%20and%20selection%20%C2%B7%20ngc7293's%20blog/" title="evaluation and selection · ngc7293's blog">evaluation and selection · ngc7293's blog</a><time datetime="2024-04-03T03:47:36.243Z" title="发表于 2024-04-03 11:47:36">2024-04-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/zl/2016-01-01-998_lua%20%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" title="lua 基础数据类型"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="lua 基础数据类型"/></a><div class="content"><a class="title" href="/zl/2016-01-01-998_lua%20%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/" title="lua 基础数据类型">lua 基础数据类型</a><time datetime="2024-04-03T03:47:36.243Z" title="发表于 2024-04-03 11:47:36">2024-04-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 糖果AIGC实验室 备案:辽ICP备16003836号-5</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><script>(function () {
  var meta = document.createElement('meta');
  meta.content = 'no-referrer';
  meta.name = 'referrer';
  document.getElementsByTagName('head')[0].appendChild(meta);
})();</script></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script></div></body></html>