{"title":"","uid":"f73a8e23e6f6f669cf99c7dba8fa0722","slug":"jinxing/不听 老人言","date":"2024-03-14T07:45:09.120Z","updated":"2024-03-14T07:45:09.121Z","comments":true,"path":"api/articles/jinxing/不听 老人言.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","content":"<p>从原理上解释一下为什么：“不听老人言，吃亏在眼前！‘</p>\n<p>从心理学角度来解释，年长者通常经历了更多的人生阅历和经验积累，他们对事物的判断和决策更为成熟和准确。他们能够通过自己的经验教训来指导年轻人，帮助他们避免犯同样的错误。</p>\n<p>此外，年长者在社会中拥有更高的地位和威信，他们的建议往往具有权威性。因此，不听从年长者的意见可能导致缺乏正确的指导，而出现失误或吃亏。</p>\n<p>另外，年长者也具备更多的人际关系网络和社会资源，他们可以通过这些资源提供帮助和支持。如果不听从他们的建议，则可能失去了得到这些资源的机会。</p>\n<p>总之，“不听老人言，吃亏在眼前”这句话表达了尊重和借鉴前辈经验教训的重要性。年长者对我们来说是宝贵的财富，在做决策时应该谦虚地听取他们的意见，并根据需要加以参考和应用。</p>\n<p>从原理上解释一下为什么：“不听老人言，开心一整年！‘</p>\n<p>“不听老人言，开心一整年！”这句话的原理可以从两个方面解释。</p>\n<p>首先，老人的经验和智慧是通过多年的生活经历积累起来的。他们在各种情况下都可能有着宝贵的建议和指导。因此，听取老人的意见可以避免重复犯错，减少风险，并且能够更快地找到解决问题的方法。如果我们不听老人言，就可能会陷入自己不必要的错误中，从而导致不必要的痛苦和困扰。</p>\n<p>其次，老人往往具有更加平静和豁达的心态。他们对生活有着更全面、更深刻的理解。通过倾听他们的故事和观点，我们可以学到如何正确处理挫折和困难，并拥有更健康、积极的心态。这样一来，在遇到问题时我们就能更加从容地面对，并且能够更好地享受生活中美好的事物。</p>\n<p>综上所述，“不听老人言，开心一整年！”这句话之所以成立，是因为老人通常具有丰富的经验和智慧，在许多情况下能够给出宝贵建议；同时也因为他们的平和心态和积极态度可以影响我们的情绪和态度，使我们更加开心和满足。因此，听取老人的意见并尊重他们是一个值得推崇的行为。</p>\n<p>三仙献鼎局</p>\n<p>三仙献鼎局是指中国古代神话传说中的一个故事，主要涉及到三位神仙献上了自己的法宝鼎给玉皇大帝。</p>\n<p>故事中，有三位神仙分别是：东华帝君、南极仙翁和西方毗卢仙君。他们都是玉皇大帝的得力助手，拥有着强大的法力和神通。</p>\n<p>这三位神仙听说玉皇大帝需要一座冰铜鼎来供奉天地之间的灵气，于是分别动身前往三江之源、南海之滨和西海之畔寻找适合铸造鼎的材料。</p>\n<p>东华帝君找到了一块源自东海深处的寒铜；南极仙翁发现了一块取自南山峰顶的冰晶；西方毗卢仙君则在西岭山脚下找到了一块火山岩。</p>\n<p>他们将这些材料带回天宫后，共同努力将这些材料熔化并铸造成了一座美丽而庄重的冰铜鼎。这座鼎上刻有精美的图案和文字，象征着天地间的和谐与祥和。</p>\n<p>最后，三位神仙将这座冰铜鼎献给了玉皇大帝。玉皇大帝赞赏他们的努力和智慧，并将这座鼎放置在天宫的重要位置上，作为供奉天地灵气的器物。</p>\n<p>三仙献鼎局这个故事寓意着人与自然的和谐相处，也强调了团结合作的重要性。同时，它也反映了中国古代文化中对于神仙、法宝等神话元素的崇拜和尊重。</p>\n<p>科斯定理</p>\n<p>科斯定理（Coase’s theorem）是由英国经济学家罗纳德·科斯（Ronald Coase）于1960年提出的一个经济学理论。该理论主要探讨了在存在外部性的情况下，市场机制如何通过私人协商来实现资源的有效配置。</p>\n<p>科斯定理认为，当市场存在外部性时，如果交易成本较低且产权明确，受到外部性影响的各方可以通过私人协商来达成一种有效的资源配置方式，而无需政府干预。换句话说，只要交易成本较低且产权得到明确和保护，市场参与者可以通过自愿交易来最大化社会福利。</p>\n<p>科斯定理的核心思想是强调了私人协商在资源配置中的重要性。根据该定理，当市场参与者面对外部性问题时，他们可以通过协商、合同或谈判等方式来解决争议，并找到一种相对最优的解决方案。这种解决方案可能包括支付补偿、签订契约或重新划分产权等形式。</p>\n<p>然而，科斯定理并不意味着私人协商一定能够实现最优资源配置。在现实世界中，交易成本可能较高，产权可能不明确或无法得到有效保护，私人协商可能受到信息不对称、谈判能力差异等因素的影响。因此，政府在某些情况下仍需要干预，通过立法、监管等手段来纠正市场失灵和改善资源配置。</p>\n<p>总之，科斯定理强调了私人协商在解决外部性问题中的作用，并提出了一种理论框架来分析资源配置问题。然而，在实践中，需要综合考虑交易成本、产权保护和政府干预等因素来找到最优的资源配置方式。</p>\n<p>帕累托最优</p>\n<p>帕累托最优是指在资源有限的情况下，通过合理分配资源，使得至少有一方的利益不受损害，而其他方的利益能够最大化。这个概念源自于意大利经济学家维尔弗雷多·帕累托（Vilfredo Pareto）提出的“帕累托改进”原则。</p>\n<p>在帕累托最优的情况下，任何一方都无法通过调整资源分配来获得更多的利益，即已经达到了最优效果。这种情况下，只有通过减少某一方的利益或者增加其他方的利益才能实现资源再分配。</p>\n<p>帕累托最优常常用于经济学和社会福利领域，例如在税收政策中，通过合理调整税收负担来实现社会公平和效率的平衡；在资源配置上，通过合理分配资源来提高整体效益等。帕累托最优原则可以作为一种评估和指导社会政策和经济决策的依据。</p>\n<p>恩格尔系数</p>\n<p>恩格尔系数是一个经济学术语，用于衡量家庭或个人消费支出中食品支出所占比例的指标。它是由德国统计学家恩格尔（Ernst Engel）于19世纪提出的。</p>\n<p>恩格尔系数的计算方法是将食品支出除以总消费支出，并乘以100，以得到一个百分比。公式如下：</p>\n<p>恩格尔系数 = (食品支出 / 总消费支出) * 100</p>\n<p>恩格尔系数越高，表示家庭或个人在消费中用于购买食品的比例越大，说明其可支配收入相对较低；反之，恩格尔系数越低，则表示家庭或个人在消费中用于购买食品的比例较小，说明其可支配收入相对较高。</p>\n<p>恩格尔系数常被用作衡量贫困程度和经济发展水平的指标。通常情况下，随着经济发展和人民生活水平的提高，恩格尔系数会逐渐降低。这是因为随着收入增加，人们可以更多地购买非必需品和奢侈品，从而减少了食品支出所占比例。</p>\n<p>然而，在一些贫困地区或低收入家庭中，恩格尔系数可能会较高，表示大部分收入被用于满足基本的食品需求。因此，恩格尔系数也可以作为贫困程度和社会经济状况的一个指标。</p>\n<p>transformer模型</p>\n<p>Transformer模型是一种基于自注意力机制的序列到序列模型，最初用于机器翻译任务。它由多个编码器和解码器层组成，每个层都包含了自注意力和前馈神经网络两个子层。</p>\n<p>在编码器中，输入序列首先通过自注意力机制进行编码，即计算每个位置与其他位置的相关性权重，从而获得每个位置的上下文表示。然后，在前馈神经网络中对上下文表示进行非线性变换。这样，编码器可以将输入序列转化为一组高维向量表示。</p>\n<p>在解码器中，目标序列的每个位置都通过自注意力机制获取其上下文信息，并与编码器的输出进行关联。然后，在前馈神经网络中对上下文表示进行非线性变换，并预测当前位置的输出。</p>\n<p>自注意力机制是Transformer模型的核心部分。它通过计算查询、键和值之间的相关性来获取位置之间的相互作用。这使得模型能够在没有显式对齐过程的情况下捕捉到长距离依赖关系。</p>\n<p>相比于传统的循环神经网络（RNN），Transformer模型具有更好的并行计算能力和更短的训练时间。它还能够处理长序列，并且不受序列长度的限制。因此，Transformer模型在自然语言处理和其他序列任务中取得了很大的成功。</p>\n<p>写一段Transformerj模型的Helloworld程序的Python代码的例子。</p>\n<p>下面是一个使用Transformer模型进行文本分类的示例代码：</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>optim <span class=\"token keyword\">as</span> optim\n<span class=\"token keyword\">from</span> torchtext<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> AG_NEWS\n<span class=\"token keyword\">from</span> torchtext<span class=\"token punctuation\">.</span>data <span class=\"token keyword\">import</span> Field<span class=\"token punctuation\">,</span> LabelField<span class=\"token punctuation\">,</span> BucketIterator\n\n<span class=\"token comment\"># 定义模型参数</span>\nembedding_dim <span class=\"token operator\">=</span> <span class=\"token number\">256</span>\nhidden_dim <span class=\"token operator\">=</span> <span class=\"token number\">512</span>\noutput_dim <span class=\"token operator\">=</span> <span class=\"token number\">4</span>\nnum_heads <span class=\"token operator\">=</span> <span class=\"token number\">8</span>\nnum_layers <span class=\"token operator\">=</span> <span class=\"token number\">6</span>\n\n<span class=\"token comment\"># 定义数据处理的Field对象</span>\ntext_field <span class=\"token operator\">=</span> Field<span class=\"token punctuation\">(</span>tokenize<span class=\"token operator\">=</span><span class=\"token string\">'spacy'</span><span class=\"token punctuation\">,</span> lower<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\nlabel_field <span class=\"token operator\">=</span> LabelField<span class=\"token punctuation\">(</span>dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 下载和处理数据集</span>\ntrain_data<span class=\"token punctuation\">,</span> test_data <span class=\"token operator\">=</span> AG_NEWS<span class=\"token punctuation\">.</span>splits<span class=\"token punctuation\">(</span>text_field<span class=\"token punctuation\">,</span> label_field<span class=\"token punctuation\">)</span>\nlabel_field<span class=\"token punctuation\">.</span>build_vocab<span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">)</span>\ntrain_iterator<span class=\"token punctuation\">,</span> test_iterator <span class=\"token operator\">=</span> BucketIterator<span class=\"token punctuation\">.</span>splits<span class=\"token punctuation\">(</span>\n    <span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">,</span> test_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    batch_size<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span>\n    device<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda'</span> <span class=\"token keyword\">if</span> torch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>is_available<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">else</span> <span class=\"token string\">'cpu'</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">TransformerModel</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">,</span> hidden_dim<span class=\"token punctuation\">,</span> output_dim<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>TransformerModel<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>embedding_dim <span class=\"token operator\">=</span> embedding_dim\n        \n        self<span class=\"token punctuation\">.</span>embedding <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Embedding<span class=\"token punctuation\">(</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>text_field<span class=\"token punctuation\">.</span>vocab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>pos_encoding <span class=\"token operator\">=</span> PositionalEncoding<span class=\"token punctuation\">(</span>embedding_dim<span class=\"token punctuation\">)</span>\n        \n        encoder_layer <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>TransformerEncoderLayer<span class=\"token punctuation\">(</span>\n            d_model<span class=\"token operator\">=</span>embedding_dim<span class=\"token punctuation\">,</span>\n            nhead<span class=\"token operator\">=</span>num_heads<span class=\"token punctuation\">,</span>\n            dim_feedforward<span class=\"token operator\">=</span>hidden_dim<span class=\"token punctuation\">,</span>\n            dropout<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span>\n            activation<span class=\"token operator\">=</span><span class=\"token string\">'relu'</span>\n        <span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>transformer_encoder <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>TransformerEncoder<span class=\"token punctuation\">(</span>encoder_layer<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\n        \n        self<span class=\"token punctuation\">.</span>fc1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>embedding_dim<span class=\"token punctuation\">,</span> hidden_dim<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>fc2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>hidden_dim<span class=\"token punctuation\">,</span> output_dim<span class=\"token punctuation\">)</span>\n        \n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> text<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        embedded_text <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>embedding<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> math<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>embedding_dim<span class=\"token punctuation\">)</span>\n        encoded_text <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>pos_encoding<span class=\"token punctuation\">(</span>embedded_text<span class=\"token punctuation\">)</span>\n        transformer_output <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>transformer_encoder<span class=\"token punctuation\">(</span>encoded_text<span class=\"token punctuation\">)</span>\n        pooled_output<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>transformer_output<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        logits <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>fc2<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>fc1<span class=\"token punctuation\">(</span>pooled_output<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token keyword\">return</span> logits\n\n<span class=\"token comment\"># 定义位置编码</span>\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">PositionalEncoding</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">,</span> max_len<span class=\"token operator\">=</span><span class=\"token number\">5000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>PositionalEncoding<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        \n        position <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> max_len<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>unsqueeze<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        div_term <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>exp<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>arange<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span>math<span class=\"token punctuation\">.</span>log<span class=\"token punctuation\">(</span><span class=\"token number\">10000.0</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> embedding_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        \n        self<span class=\"token punctuation\">.</span>position_encoding <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>max_len<span class=\"token punctuation\">,</span> embedding_dim<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>position_encoding<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>sin<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>position_encoding<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">:</span><span class=\"token number\">2</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>cos<span class=\"token punctuation\">(</span>position <span class=\"token operator\">*</span> div_term<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x <span class=\"token operator\">+=</span> self<span class=\"token punctuation\">.</span>position_encoding<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>x<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span>\n        \n        <span class=\"token keyword\">return</span> x\n\n<span class=\"token comment\"># 初始化模型</span>\nmodel <span class=\"token operator\">=</span> TransformerModel<span class=\"token punctuation\">(</span>embedding_dim<span class=\"token punctuation\">,</span> hidden_dim<span class=\"token punctuation\">,</span> output_dim<span class=\"token punctuation\">,</span> num_heads<span class=\"token punctuation\">,</span> num_layers<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 定义损失函数和优化器</span>\ncriterion <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> optim<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 训练模型</span>\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>num_epochs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    model<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">for</span> batch <span class=\"token keyword\">in</span> train_iterator<span class=\"token punctuation\">:</span>\n        optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        \n        text <span class=\"token operator\">=</span> batch<span class=\"token punctuation\">.</span>text\n        label <span class=\"token operator\">=</span> batch<span class=\"token punctuation\">.</span>label\n        \n        output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n        \n        loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> label<span class=\"token punctuation\">)</span>\n        \n        loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        \n        optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># 测试模型</span>\nmodel<span class=\"token punctuation\">.</span><span class=\"token builtin\">eval</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> batch <span class=\"token keyword\">in</span> test_iterator<span class=\"token punctuation\">:</span>\n        text <span class=\"token operator\">=</span> batch<span class=\"token punctuation\">.</span>text\n        label <span class=\"token operator\">=</span> batch<span class=\"token punctuation\">.</span>label\n        \n        output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># 进行预测等操作</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n<p>注意：上述代码中使用了torchtext库来加载和处理AG_NEWS数据集，如果没有安装该库，可以使用其他文本数据集或手动处理数据。另外，代码中的一些参数（如num_epochs）需要根据实际情况进行调整。</p>\n","text":"从原理上解释一下为什么：“不听老人言，吃亏在眼前！‘ 从心理学角度来解释，年长者通常经历了更多的人生阅历和经验积累，他们对事物的判断和决策更为成熟和准确。他们能够通过自己的经验教训来指导年轻人，帮助他们避免犯同样的错误。 此外，年长者在社会中拥有更高的地位和威信，他们的建议往往具...","link":"","photos":[],"count_time":{"symbolsCount":"6.4k","symbolsTime":"6 mins."},"categories":[],"tags":[],"toc":"","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"","uid":"f73a8e23e6f6f669cf99c7dba8fa0722","slug":"jinxing/USB3 与SD存储卡之间的速度差异","date":"2024-03-14T07:45:09.120Z","updated":"2024-03-14T07:45:09.120Z","comments":true,"path":"api/articles/jinxing/USB3 与SD存储卡之间的速度差异.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":"USB3 .0 与SD存储卡之间的读取速度差多少？ USB3.0与SD存储卡之间的读取速度差异较大，主要取决于具体的设备和存储卡规格。一般来说，USB3.0接口的读取速度可以达到每秒5Gbps，而SD存储卡的读取速度则根据不同的规格和等级有所不同。 相对而言，SD存储卡的读取速度...","link":"","photos":[],"count_time":{"symbolsCount":"1.3k","symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Perplexity宣布B轮7360万美元融资","uid":"cb5fe6c2b1cfab8a63443052c075ad8b","slug":"news/Perplexity宣布B轮7360万美元融资","date":"2024-03-14T07:45:09.120Z","updated":"2024-03-14T07:45:09.120Z","comments":true,"path":"api/articles/news/Perplexity宣布B轮7360万美元融资.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":" Perplexity宣布B轮7360万美元融资 AI搜索工具Perplexity宣布B轮7360万美元融资，估值达到5.2亿美元。 下面是这轮融资的一些信息： Perplexity的月活跃用户增长到了1000万，并在2023年处理了超过50亿次查询。 iOS和Android应用...","link":"","photos":[],"count_time":{"symbolsCount":449,"symbolsTime":"1 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":117,"path":"api/categories/AIGC.json"},{"name":"news","slug":"AIGC/news","count":1,"path":"api/categories/AIGC/news.json"}],"tags":[{"name":"news","slug":"news","count":1,"path":"api/tags/news.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}