{"title":"Sora 和之前 Runway 那些在架构上有啥区别呢","uid":"d513b66cf42e1723c2bda6d425c5b9ed","slug":"aigc/sora/Sora 和之前 Runway 那些在架构上有啥区别呢","date":"2024-03-14T06:15:59.711Z","updated":"2024-03-14T06:15:59.711Z","comments":true,"path":"api/articles/aigc/sora/Sora 和之前 Runway 那些在架构上有啥区别呢.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":[],"content":"<h1 id=\"Sora-和之前-Runway-那些在架构上有啥区别呢\"><a href=\"#Sora-和之前-Runway-那些在架构上有啥区别呢\" class=\"headerlink\" title=\"Sora 和之前 Runway 那些在架构上有啥区别呢\"></a>Sora 和之前 Runway 那些在架构上有啥区别呢</h1><p>Sora是基于Diffusion Transformer模型的生成式模型，融合了扩散模型和Transformer架构，能有效处理含噪点的图像输入并逐步预测出更清晰的图像版本。与传统Token预测不同，Sora预测序列中的下一个Patch，使OpenAI在处理大规模图像和视频数据时取得显著进展。由于基于Patch而非全帧训练，Sora无需裁剪任何大小的视频或图片，输出质量更高。结合Diffusion Transformer架构，OpenAI为训练Sora倾注更多数据和资源，取得惊人效果。</p>\n<p>问：Sora 和之前 Runway 那些在架构上有啥区别呢？  </p>\n<p>答：简单来说 Runway 是基于扩散模型（Diffusion Model）的，而 Sora 是基于 Diffusion Transformer。  </p>\n<p>Runway、Stable Diffusion 是基于扩散模型（Diffusion Model），扩散模型（Diffusion Model）的训练过程是通过多个步骤逐渐向图片增加噪点，直到图片变成完全无结构的噪点图片，然后在生成图片的时候，基于一张完全噪点的图片，逐步减少噪点，直到还原出一张清晰的图片。  </p>\n<p>文本模型像 GPT-4 则是 Transformer 模型。Transformer 则是一套编码器和解码器的架构，将文本编码成数字向量，然后解码的时候从数字向量还原出文本。  </p>\n<p>Sora 则是一个融合了两者的 Diffusion Transformer 模型。通过 Transformer 的编码器 - 解码器架构处理含噪点的输入图像，并在每一步预测出更清晰的图像版本。编码器负责对含噪点的输入进行编码，而解码器则负责生成更清晰图像的预测。  </p>\n<p>GPT-4 被训练以处理一串 Token，并预测出下一个 Token。Sora 不是预测序列中的下一个文本，而是预测序列中的下一个“Patch”。  </p>\n<p>在文本预测生成中，基本单位是 Token，Token 很好理解，就是一个单词或者单词的一部分。Patch 的概念相对不那么好理解，不过今天看到一篇文章，作者举了个很好的例子。  </p>\n<p>想象一下《黑暗骑士》的电影胶片，将一卷胶片绕在一个金属盘上，然后挂在一个老式电影院的投影机上。  </p>\n<p>你把电影胶卷从盘中展开，然后剪下最前面的 100 帧。你挑出每一帧——这里是小丑疯狂大笑，那里是蝙蝠侠痛苦的表情——并进行以下不同寻常的操作：  </p>\n<p>你拿起一把 X-acto 精细刻刀，在第一帧电影胶片上剪出一个变形虫状的图案。你像处理精密仪器一样小心翼翼地用镊子提取这片形似变形虫的胶片，然后安全地保存起来。之后，你处理下一帧：在接下来的胶片上切出同样位置、同样形状的变形虫图案。你再次用镊子小心地取出这个新的变形虫形状的胶片——形状与前一个完全相同——并将其精确地放置在第一个之上。你这样做，直到完成所有的 100 帧。  </p>\n<p>你现在有了一个色彩斑斓的变形虫，沿着 Y 轴扩展。这是一座可以通过投影机播放《黑暗骑士》的小片段的胶片塔，就好像有人在投影机前握着拳头，只让电影的一小部分影像从拳心通过。  </p>\n<p>然后，这座胶片塔被压缩并转化为所谓的“Patch”——一种随时间变化的色块。  </p>\n<p>Patch 的创新之处——以及 Sora 之所以显得如此强大——在于它们让 OpenAI 能够在大量的图像和视频数据上训</p>\n<p>练模型，从而生成更清晰、更真实的图像。通过融合扩散模型和Transformer架构，Sora能够有效处理含噪点的图像输入，并逐步预测出更清晰的图像版本。与传统的Token预测不同，Sora预测序列中的下一个Patch，这种创新使得OpenAI在处理大规模图像和视频数据时取得了显著进展。通过将两种不同的架构结合在一起，Sora展现了强大的生成能力和潜力。练 Sora。想象一下从每一个存在的视频中剪出的 Patch——无尽的胶片塔——被堆叠起来并输入到模型中。  </p>\n<p>以前的文本转视频方法需要训练时使用的所有图片和视频都要有相同的大小，这就需要大量的预处理工作来裁剪视频至适当的大小。但是，由于 Sora 是基于“Patch”而非视频的全帧进行训练的，它可以处理任何大小的视频或图片，无需进行裁剪。  </p>\n<p>因此，可以有更多的数据用于训练，得到的输出质量也会更高。例如，将视频预处理至新的长宽比通常会导致视频的原始构图丢失。一个在宽屏中心呈现人物的视频，裁剪后可能只能部分展示该人物。因为 Sora 能接收任何视频作为训练输入，所以其输出不会受到训练输入构图不良的影响。  </p>\n<p>在结合前面提到的 Diffusion Transformer 架构，OpenAI 可以在训练 Sora 时倾注更多的数据和计算资源，从而得到令人惊叹的效果。  </p>\n<p>另外 Sora 刚发布视频时，能模拟出咖啡在杯子里溅出的液体动力学，以至于有人以为是连接了游戏引擎，但实际上 Sora 还是基于生成式模型，这是因为 Sora 在训练时，使用了大量的视频数据，这些视频中包含了大量的物理规则，所以 Sora 能够模拟出液体动力学。这类似于 GPT-4 在训练时，使用了大量的代码来作为训练数据，所以 GPT-4 能够生成代码。  </p>\n<p>有两篇论文：<br>《Scalable Diffusion Models with Transformers》<a href=\"https://weibo.cn/sinaurl?u=https://arxiv.org/abs/2212.09748\"><img src=\"http://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png\">网页链接</a><br>《Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution》<a href=\"https://weibo.cn/sinaurl?u=https://arxiv.org/abs/2307.06304\"><img src=\"http://h5.sinaimg.cn/upload/2015/09/25/3/timeline_card_small_web_default.png\">网页链接</a>  </p>\n<p>包含更多专业细节。</p>\n","text":"Sora 和之前 Runway 那些在架构上有啥区别呢Sora是基于Diffusion Transformer模型的生成式模型，融合了扩散模型和Transformer架构，能有效处理含噪点的图像输入并逐步预测出更清晰的图像版本。与传统Token预测不同，Sora预测序列中的下一个...","link":"","photos":[],"count_time":{"symbolsCount":"2.3k","symbolsTime":"2 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":119,"path":"api/categories/AIGC.json"},{"name":"sora","slug":"AIGC/sora","count":1,"path":"api/categories/AIGC/sora.json"}],"tags":[{"name":"sora","slug":"sora","count":2,"path":"api/tags/sora.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Sora-%E5%92%8C%E4%B9%8B%E5%89%8D-Runway-%E9%82%A3%E4%BA%9B%E5%9C%A8%E6%9E%B6%E6%9E%84%E4%B8%8A%E6%9C%89%E5%95%A5%E5%8C%BA%E5%88%AB%E5%91%A2\"><span class=\"toc-text\">Sora 和之前 Runway 那些在架构上有啥区别呢</span></a></li></ol>","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"《我们这一代人的机会是什么？》","uid":"94a35c942b1e4d4bd9646732d214e01d","slug":"article/《我们这一代人的机会是什么？》","date":"2024-03-14T06:15:59.711Z","updated":"2024-03-14T06:15:59.711Z","comments":true,"path":"api/articles/article/《我们这一代人的机会是什么？》.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":[],"text":"《我们这一代人的机会是什么？》作者：Rey 1998年前后，搜狐新浪网易、BAT相继成立，中国互联网时代开启，伴随而来的还有风险投资； 1998年事业单位停止福利分房，开启房地产市场化时代； 1998年亚洲金融危机，中国增发特别国债加强基建，开始大规模基础设施建设时代，拉动经济增...","link":"","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":119,"path":"api/categories/AIGC.json"},{"name":"weibo","slug":"AIGC/weibo","count":59,"path":"api/categories/AIGC/weibo.json"}],"tags":[{"name":"weibo","slug":"weibo","count":62,"path":"api/tags/weibo.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"宇宙探索编辑部-关于Sora讨论","uid":"8498990ea0518320f72ae9f350cc7d26","slug":"aigc/sora/宇宙探索编辑部-关于Sora讨论","date":"2024-03-14T06:15:59.711Z","updated":"2024-03-14T06:15:59.711Z","comments":true,"path":"api/articles/aigc/sora/宇宙探索编辑部-关于Sora讨论.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"text":"宇宙探索编辑部-关于Sora讨论很荣幸受 邀请，今天和她以及《宇宙探索编辑部》副导演吕启洋（Ash）一起聊聊了一下当前火爆的话题 Sora，看 Sora 如何改变我们的生活。 我把技术相关的一些问题整理成了文字，希望能够帮助大家更好地理解 Sora。我将问题大约整理成了四类： S...","link":"","photos":[],"count_time":{"symbolsCount":"4k","symbolsTime":"4 mins."},"categories":[{"name":"笔记","slug":"笔记","count":1,"path":"api/categories/笔记.json"}],"tags":[{"name":"sora","slug":"sora","count":2,"path":"api/tags/sora.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}