{"title":"ChatGLM2如何使用多显卡GPU","uid":"db337d0c1275ad3ba82eaea7216e78e0","slug":"kanbanfiles/ChatGLM2如何使用多显卡GPU","date":"2024-03-14T07:45:09.039Z","updated":"2024-03-14T07:45:09.039Z","comments":true,"path":"api/articles/kanbanfiles/ChatGLM2如何使用多显卡GPU.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"content":"<h2 id=\"ChatGLM2的train-sh的NUM-GPUS如何设置配置双显卡？\">ChatGLM2的train.sh的NUM_GPUS如何设置配置双显卡？</h2>\n<p>ChatGLM和ChatGLM2，对GPU配置的传参数样式不太一样，一代的时候，“0,1”这么用双显卡，二代的时候直接输入NUM_GPUS=2.</p>\n<p>要配置双显卡进行训练，可以将NUM_GPUS设置为2，以表示使用两个GPU。以下是一个示例的train.sh文件的内容：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> NUM_GPUS=2</span><br><span class=\"line\"></span><br><span class=\"line\">python train.py \\</span><br><span class=\"line\">  --data_dir=data \\</span><br><span class=\"line\">  --model_dir=model \\</span><br><span class=\"line\">  --num_gpus=<span class=\"variable\">$NUM_GPUS</span> \\</span><br><span class=\"line\">  --batch_size=64 \\</span><br><span class=\"line\">  --learning_rate=0.001</span><br></pre></td></tr></table></figure>\n<p>在这个示例中，将NUM_GPUS设置为2，并传递给train.py脚本的–num_gpus参数。根据你的需求，你可以调整其他参数，例如数据目录、模型目录、批量大小和学习率等。</p>\n<p>请注意，具体的命令可能会根据ChatGLM2的实际代码库有所不同，请根据你使用的代码库和框架进行相应调整。</p>\n<h2 id=\"ChatGLM2如何使用多显卡GPU\">ChatGLM2如何使用多显卡GPU</h2>\n<p>要在ChatGLM2中使用多个显卡GPU，你需要使用分布式训练的方法。以下是一个使用多个显卡GPU进行ChatGLM2训练的示例：</p>\n<ol>\n<li class=\"lvl-3\">\n<p>确保你的计算机上有多个显卡GPU，并安装了相应的驱动程序。</p>\n</li>\n<li class=\"lvl-3\">\n<p>使用<code>transformers</code>库加载ChatGLM2模型和tokenizer：</p>\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> ChatGPT</span><br><span class=\"line\"></span><br><span class=\"line\">model = ChatGPT.from_pretrained(<span class=\"string\">&quot;microsoft/DialoGPT-medium&quot;</span>)</span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"string\">&quot;microsoft/DialoGPT-medium&quot;</span>)</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li class=\"lvl-3\">\n<p>使用<code>torch.nn.DataParallel</code>将模型包装在多个GPU上：</p>\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span>)</span><br><span class=\"line\">model = nn.DataParallel(model)</span><br><span class=\"line\">model.to(device)</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li class=\"lvl-3\">\n<p>在训练循环中，确保将输入数据和目标标签移动到正确的设备上：</p>\n</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">input_ids = input_ids.to(device)</span><br><span class=\"line\">attention_mask = attention_mask.to(device)</span><br><span class=\"line\">labels = labels.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)</span><br></pre></td></tr></table></figure>\n<p>这样，模型就会自动在多个显卡GPU上并行计算，并共享梯度更新。</p>\n<p>请注意，使用多个显卡GPU进行训练可能需要更大的批次大小和更长的训练时间。此外，还可以通过设置CUDA_VISIBLE_DEVICES环境变量来选择要使用的特定GPU设备。</p>\n","text":"ChatGLM2的train.sh的NUM_GPUS如何设置配置双显卡？ ChatGLM和ChatGLM2，对GPU配置的传参数样式不太一样，一代的时候，“0,1”这么用双显卡，二代的时候直接输入NUM_GPUS=2. 要配置双显卡进行训练，可以将NUM_GPUS设置为2，以表示...","link":"","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"ChatGLM","slug":"ChatGLM","count":1,"path":"api/categories/ChatGLM.json"}],"tags":[{"name":"ChatGLM2","slug":"ChatGLM2","count":1,"path":"api/tags/ChatGLM2.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#ChatGLM2%E7%9A%84train-sh%E7%9A%84NUM-GPUS%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E9%85%8D%E7%BD%AE%E5%8F%8C%E6%98%BE%E5%8D%A1%EF%BC%9F\"><span class=\"toc-text\">ChatGLM2的train.sh的NUM_GPUS如何设置配置双显卡？</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#ChatGLM2%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%A4%9A%E6%98%BE%E5%8D%A1GPU\"><span class=\"toc-text\">ChatGLM2如何使用多显卡GPU</span></a></li></ol>","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"cuDNN","uid":"95e90514163f2adead82ccfd062b2e72","slug":"python/codna/cuDNN","date":"2024-03-14T07:45:09.040Z","updated":"2024-03-14T07:45:09.040Z","comments":true,"path":"api/articles/python/codna/cuDNN.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"text":"cuDNN cuDNN（CUDA Deep Neural Network library）是一个用于加速深度神经网络计算的GPU加速库。它提供了高效的实现和优化，可以显著加快深度学习模型训练和推理的速度。 使用cuDNN，可以进行以下操作： 卷积神经网络（CNN）：cuDNN提供...","link":"","photos":[],"count_time":{"symbolsCount":"1.1k","symbolsTime":"1 mins."},"categories":[{"name":"AIGC","slug":"AIGC","count":119,"path":"api/categories/AIGC.json"},{"name":"cuDNN","slug":"AIGC/cuDNN","count":1,"path":"api/categories/AIGC/cuDNN.json"}],"tags":[{"name":"cuDNN","slug":"cuDNN","count":1,"path":"api/tags/cuDNN.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Linux加载磁盘","uid":"846015a39439fe7ecae742a0ab7587b9","slug":"kanbanfiles/Linux加载磁盘","date":"2024-03-14T07:45:09.039Z","updated":"2024-03-14T07:45:09.039Z","comments":true,"path":"api/articles/kanbanfiles/Linux加载磁盘.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":null,"text":"0x01 概述 在Linux下如何加载一个分区，需要几个过程。 0x02 操作过程 查看分区 |[root@lua.ren~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/vda1 40G 2.2G 36G 6% ...","link":"","photos":[],"count_time":{"symbolsCount":"2.9k","symbolsTime":"3 mins."},"categories":[{"name":"linux","slug":"linux","count":7,"path":"api/categories/linux.json"}],"tags":[{"name":"linux","slug":"linux","count":4,"path":"api/tags/linux.json"},{"name":"mount","slug":"mount","count":1,"path":"api/tags/mount.json"},{"name":"磁盘","slug":"磁盘","count":1,"path":"api/tags/磁盘.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}