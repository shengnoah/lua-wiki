{"title":"Evaluation metrics for generation based models in NLP","uid":"31351d37c5d4d33e27cfeb4ae2729624","slug":"zl/2016-01-01-160_Evaluation metrics for generation based models in ","date":"2024-04-03T03:47:33.052Z","updated":"2024-04-03T03:47:33.052Z","comments":true,"path":"api/articles/zl/2016-01-01-160_Evaluation metrics for generation based models in .json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","content":"<h2 id=\"Evaluation-metrics-for-generation-based-models-in-NLP\"><a href=\"#Evaluation-metrics-for-generation-based-models-in-NLP\" class=\"headerlink\" title=\"Evaluation metrics for generation based models in NLP\"></a>Evaluation metrics for generation based models in NLP</h2><h3 id=\"BLEU\"><a href=\"#BLEU\" class=\"headerlink\" title=\"BLEU\"></a>BLEU</h3><p>转自： <a href=\"http://www.cnblogs.com/by-dream/p/7679284.html\" target=\"_blank\" rel=\"noopener noreferrer\">http://www.cnblogs.com/by-dream/p/7679284.html</a> </p>\n<h4 id=\"n-gram\"><a href=\"#n-gram\" class=\"headerlink\" title=\"n-gram\"></a>n-gram</h4><p>BLEU 采用一种N-gram的匹配规则，原理比较简单，就是比较译文和参考译文之间n组词的相似的一个占比。</p>\n<p>　　例如：</p>\n<p>　　　　原文：今天天气不错</p>\n<p>　　　　机器译文：It is a nice day today</p>\n<p>　　　　人工译文：Today is a nice day</p>\n<p>　　如果用1-gram匹配的话：</p>\n<p>　　<img src=\"https://img.dazhuanlan.com/2019/11/27/5dde01ad1ab7c.png\" alt=\"BLEU1\"/></p>\n<p>　　可以看到机器译文一共6个词，有5个词语都命中的了参考译文，那么它1-gram的匹配度为 5/6 </p>\n<p>　　我们再以3-gram举例：</p>\n<p>​    <img src=\"https://img.dazhuanlan.com/2019/11/27/5dde01ada9fe1.png\" alt=\"BLEU2\"/></p>\n<p>　　可以看到机器译文一共可以分为四个3-gram的词组，其中有两个可以命中参考译文，那么它3-gram的匹配度为 2/4  </p>\n<p>　　依次类推，我们可以很容易实现一个程序来遍历计算N-gram的一个匹配度。一般来说1-gram的结果代表了文中有多少个词被单独翻译出来了，因此它反映的是这篇译文的忠实度；而当我们计算2-gram以上时，更多时候结果反映的是译文的流畅度，值越高文章的可读性就越好。</p>\n<h4 id=\"召回率修正\"><a href=\"#召回率修正\" class=\"headerlink\" title=\"召回率修正\"></a>召回率修正</h4><p>面所说的方法比较好理解，也比较好实现，但是没有考虑到召回率，举一个非常简单的例子说明：</p>\n<p>　　原文：猫站在地上</p>\n<p>　　机器译文：the the the the </p>\n<p>　　人工译文：The cat is standing on the ground</p>\n<p>在计算1-gram的时候，the 都出现在译文中，因此匹配度为4/4 ，但是很明显 the 在人工译文中最多出现的次数只有2次，因此BLEU算法修正了这个值的算法，首先会计算该n-gram在译文中可能出现的最大次数：</p>\n<p>$$Count_{clip}=min(Count, Max_ref_count)$$</p>\n<p>Count是N-gram在机器翻译译文中的出现次数，$Max_Ref_Count$是该N-gram在一个参考译文中最大的出现次数，最终统计结果取两者中的较小值。然后在把这个匹配结果除以机器翻译译文的N-gram个数。因此对于上面的例子来说，修正后的1-gram的统计结果就是2/4。</p>\n<p>我们将整个要处理的将机器翻译的句子表示为$C_{i}$，标准答案表示为 $S_{i}=s_{i1},…,s_{im}$（m表示有m个参考答案）　　</p>\n<p>　　n-grams表示n个单词长度的词组集合，令$W_{k}$第k个n-gram</p>\n<p>　　比如这样的一句话，”I come from china”，<em>第1个2-gram为：I come; 第2个2-gram为：come from; 第3个2-gram为：from china;</em></p>\n<p>　　$H_{k}(C_{i})$ 表示$W_{k}$翻译选译文$C_{i}$中出现的次数</p>\n<p>　　$H_{k}(S_{ij})$ 表示$W_{k}$在标准答案$S_{ij}$中出现的次数</p>\n<p>所以各阶n-gram的精度可以用下面的公式计算：</p>\n<p>$$P_{n} = frac{sum_{i}sum_{k}min(H_{k}(C_{i}, max_{jin m}H_{k}(S_{ij})))}{sum_{i}sum_{k}min(H_{k}(C_{i}))}$$</p>\n<h4 id=\"惩罚因子\"><a href=\"#惩罚因子\" class=\"headerlink\" title=\"惩罚因子\"></a>惩罚因子</h4><p>　　上面的算法已经足够可以有效的翻译评估了，然而N-gram的匹配度可能会随着句子长度的变短而变好，因此会存在这样一个问题：一个翻译引擎只翻译出了句子中部分句子且翻译的比较准确，那么它的匹配度依然会很高。为了避免这种评分的偏向性，BLEU在最后的评分结果中引入了长度惩罚因子(Brevity Penalty)。</p>\n<p></p>\n<p>$l_{c}$代表表示机器翻译译文的长度，$l_{s}$表示参考答案的有效长度，当存在多个参考译文时，选取和翻译译文最接近的长度。当翻译译文长度大于参考译文的长度时，惩罚系数为1，意味着不惩罚，只有机器翻译译文长度小于参考答案才会计算惩罚因子。</p>\n<h4 id=\"BLEU-1\"><a href=\"#BLEU-1\" class=\"headerlink\" title=\"BLEU\"></a>BLEU</h4><p>由于各N-gram统计量的精度随着阶数的升高而呈指数形式递减，所以为了平衡各阶统计量的作用，对其采用几何平均形式求平均值然后加权，再乘以长度惩罚因子，得到最后的评价公式：</p>\n<p>$$BLEU = BP× exp(sum_{n=1}^{N}W_{n}logP_{n})$$</p>\n<p>其中，$W_{n}=1/n$，N的取值最大为4。</p>\n<h3 id=\"ROUGE\"><a href=\"#ROUGE\" class=\"headerlink\" title=\"ROUGE\"></a>ROUGE</h3><blockquote>\n<p>Rouge(Recall-Oriented Understudy for Gisting Evaluation) 一种基于召回率的相似性度量方法。</p>\n</blockquote>\n<h4 id=\"ROUGE-N\"><a href=\"#ROUGE-N\" class=\"headerlink\" title=\"ROUGE-N\"></a>ROUGE-N</h4><p>$$ROUGE-N = frac{sum_{Sin{ReferencesSummaries}sum_{gram_{n}in S}Count_{match}(gram_{n})}}{sum_{Sin{ReferencesSummaries}sum_{gram_{n}in S}Count(gram_{n})}}$$</p>\n<p>n代表n元组，$Count_{match}(gram_{n})$是待评测句子中出现的最大匹配的n-grams的个数，从分子中可以看出ROUGE-N是一个基于召回率的指标</p>\n<h4 id=\"ROUGE-L\"><a href=\"#ROUGE-L\" class=\"headerlink\" title=\"ROUGE-L\"></a>ROUGE-L</h4><p>L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rouge-L使用了最长公共子序列。</p>\n<p>$$R_{cls} = frac{LCS(X,Y)}{m}$$</p>\n<p>$$P_{cls}=frac{LCS(X,Y)}{n}$$</p>\n<p>$$F_{cls}=frac{(1+beta^{2}R_{cls}P_{cls})}{R_{cls}+beta^{2}P_{cls}}$$</p>\n<p>其中$LCS(X, Y)$是X和Y的最长公共子序列的长度，m，n分别表示参考摘要和自动摘要的长度$R_{cls}$和$P_{cls}$分别表示召回率和率，$精确F_{cls}$就是我们的ROUGE-L</p>\n<table>\n<thead>\n<tr>\n<th>ROUGE-N</th>\n<th>基于n-gram共现性统计</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>ROUGE-L</td>\n<td>基于最长共有子序列共现性精确度和召回率Fmeasure统计</td>\n</tr>\n<tr>\n<td>ROUGE-W</td>\n<td>带权重的最长共有子序列共现性精确度和召回率Fmeasure统计</td>\n</tr>\n<tr>\n<td>ROUGE-S</td>\n<td>不连续二元组共现性精确度和召回率Fmeasure统计</td>\n</tr>\n</tbody>\n</table>","text":"Evaluation metrics for generation based models in NLPBLEU转自： http://www.cnblogs.com/by-dream/p/7679284.html n-gramBLEU 采用一种N-gram的匹配规则，原理比较简...","link":"","photos":[],"count_time":{"symbolsCount":"2.6k","symbolsTime":"2 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Evaluation-metrics-for-generation-based-models-in-NLP\"><span class=\"toc-text\">Evaluation metrics for generation based models in NLP</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#BLEU\"><span class=\"toc-text\">BLEU</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#n-gram\"><span class=\"toc-text\">n-gram</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E5%8F%AC%E5%9B%9E%E7%8E%87%E4%BF%AE%E6%AD%A3\"><span class=\"toc-text\">召回率修正</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#%E6%83%A9%E7%BD%9A%E5%9B%A0%E5%AD%90\"><span class=\"toc-text\">惩罚因子</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#BLEU-1\"><span class=\"toc-text\">BLEU</span></a></li></ol></li><li class=\"toc-item toc-level-3\"><a class=\"toc-link\" href=\"#ROUGE\"><span class=\"toc-text\">ROUGE</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#ROUGE-N\"><span class=\"toc-text\">ROUGE-N</span></a></li><li class=\"toc-item toc-level-4\"><a class=\"toc-link\" href=\"#ROUGE-L\"><span class=\"toc-text\">ROUGE-L</span></a></li></ol></li></ol></li></ol>","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"golua虚拟机的使用_linux 应用_黑光技术","uid":"e4e3a037cf04e3d0b6725beb21862025","slug":"zl/2016-01-01-164_golua虚拟机的使用_linux 应用_黑光技术","date":"2024-04-03T03:47:33.053Z","updated":"2024-04-03T03:47:33.053Z","comments":true,"path":"api/articles/zl/2016-01-01-164_golua虚拟机的使用_linux 应用_黑光技术.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":" &lt;/div&gt; &lt;div class=&quot;article_content&quot;&gt; &lt;p&gt;&lt;/p&gt;&lt;h1&gt;前言&lt;/h1&gt; ​ 之前一直想把openflow这样的分布式流程系统做起来，但是时间和应用...","link":"","photos":[],"count_time":{"symbolsCount":"3.7k","symbolsTime":"3 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Lua学习笔记(1) 元表","uid":"de2364008b06586313c5158e78472b93","slug":"zl/2016-01-01-162_Lua学习笔记(1) 元表","date":"2024-04-03T03:47:33.052Z","updated":"2024-04-03T03:47:33.052Z","comments":true,"path":"api/articles/zl/2016-01-01-162_Lua学习笔记(1) 元表.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":"[TOC] 基本 1. 概念 定义：Lua 中的每个值都可以用一个 metatable。 这个 metatable 就是一个原始的 Lua table ， 它用来定义原始值在特定操作下的行为。 你可以通过在 metatable 中的特定域设一些值来改变拥有这个 metatable...","link":"","photos":[],"count_time":{"symbolsCount":"2.6k","symbolsTime":"2 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}