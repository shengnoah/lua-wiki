{"title":"sketchnotes from twiml ai：evaluating model explainability methods with sara hooker","uid":"0196553ea3808d289bb4f32bd1aefeb9","slug":"zl/2016-01-01-1034_sketchnotes from twiml aievaluating model explai","date":"2024-04-03T03:47:32.974Z","updated":"2024-04-03T03:47:32.975Z","comments":true,"path":"api/articles/zl/2016-01-01-1034_sketchnotes from twiml aievaluating model explai.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","content":"<p>These are my sketchnotes for Sam Charrington’s podcast <strong>This Week in Machine Learning and AI</strong> about <em>Evaluating Model Explainability Methods with Sara Hooker</em>:</p>\n<p><img src=\"https://i2.wp.com/shiring.github.io/netlify_images/twimlai189.jpg?w=456&amp;ssl=1\" alt=\"\"/></p>\n<p>Sketchnotes from TWiMLAI talk: Evaluating Model Explainability Methods with Sara Hooker</p>\n<p>You can listen to the podcast here.</p>\n<p>In this, the first episode of the Deep Learning Indaba series, we’re joined by Sara Hooker, AI Resident at Google Brain. I had the pleasure of speaking with Sara in the run-up to the Indaba about her work on interpretability in deep neural networks. We discuss what interpretability means and when it’s important, and explore some nuances like the distinction between interpreting model decisions vs model function. We also dig into her paper Evaluating Feature Importance Estimates and look at the relationship between this work and interpretability approaches like LIME. We also talk a bit about Google, in particular, the relationship between Brain and the rest of the Google AI landscape and the significance of the recently announced Google AI Lab in Accra, Ghana, being led by friend of the show Moustapha Cisse. And, of course, we chat a bit about the Indaba as well. https://twimlai.com/twiml-talk-189-evaluating-model-explainability-methods-with-sara-hooker/</p>\n<blockquote>\n  <p>In this, the first episode of the Deep Learning Indaba series, we’re joined by Sara Hooker, AI Resident at Google Brain. I had the pleasure of speaking with Sara in the run-up to the Indaba about her work on interpretability in deep neural networks. We discuss what interpretability means and when it’s important, and explore some nuances like the distinction between interpreting model decisions vs model function. We also dig into her paper Evaluating Feature Importance Estimates and look at the relationship between this work and interpretability approaches like LIME. We also talk a bit about Google, in particular, the relationship between Brain and the rest of the Google AI landscape and the significance of the recently announced Google AI Lab in Accra, Ghana, being led by friend of the show Moustapha Cisse. And, of course, we chat a bit about the Indaba as well. https://twimlai.com/twiml-talk-189-evaluating-model-explainability-methods-with-sara-hooker/</p>\n</blockquote>\n<p><em>Related</em></p>\n<hr/>","text":"These are my sketchnotes for Sam Charrington’s podcast This Week in Machine Learning and AI about Evaluating Model Explainability Methods wi...","link":"","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"toc":"","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"how quickly do stock market valuations revert back to their means?","uid":"05e67446f684ebb6e1414fb77b2faf36","slug":"zl/2016-01-01-1033_how quickly do stock market valuations revert back","date":"2024-04-03T03:47:32.974Z","updated":"2024-04-03T03:47:32.974Z","comments":true,"path":"api/articles/zl/2016-01-01-1033_how quickly do stock market valuations revert back.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":"Mean reversion is the assumption that things tend to revert back to their means in the long run. This is especially true for valuations and ...","link":"","photos":[],"count_time":{"symbolsCount":"2k","symbolsTime":"2 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"txlualesson","uid":"6e9dc412f4edfe504d0070d0e741f221","slug":"zl/2016-01-01-1030_txlualesson","date":"2024-04-03T03:47:32.973Z","updated":"2024-04-03T03:47:32.973Z","comments":true,"path":"api/articles/zl/2016-01-01-1030_txlualesson.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":" TXLuaLesson &lt;/div&gt; ","link":"","photos":[],"count_time":{"symbolsCount":43,"symbolsTime":"1 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}