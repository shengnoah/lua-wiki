{"title":"spark rdd, pipeline, lazyevaluation","uid":"dee1c28d2a4573e95dc2faf24be94678","slug":"zl/2016-01-01-126_spark rdd, pipeline, lazyevaluation","date":"2024-04-03T03:47:33.031Z","updated":"2024-04-03T03:47:33.031Z","comments":true,"path":"api/articles/zl/2016-01-01-126_spark rdd, pipeline, lazyevaluation.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","content":"<p>一直以来写代码不求甚解，感觉这样不好，从今天开始起读各数据框架的源代码，学习学习再学习  </p>\n<p>今天看的是pyspark里lazy evaluation的处理，python和scala不同不是函数式的。那这是怎么办到的呢？  </p>\n<p>首先所有的数据集在spark内部都叫做rdd，这在pyspark里也有定义：  </p>\n<pre><code><span class=\"keyword\">class</span> RDD(object):\n<pre><code>&lt;span class=&quot;string&quot;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&amp;#34;\nA Resilient Distributed Dataset (RDD), the basic abstraction &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; Spark.\nRepresents &lt;span class=&quot;keyword&quot;&gt;an&lt;/span&gt; immutable, partitioned collection of elements that can be\noperated &lt;span class=&quot;keyword&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; parallel.\n</code></pre>\n<p></code></pre><p>RDD内部实现了很多函数，有map，filter这类一个集合对一个集合的映射，也有collect，reduce这种一个集合到一个值的映射。</p></p>\n<pre><code><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"params\">(self, f, preservesPartitioning=False)</span>:</span>\n    <span class=\"string\">&#34;&#34;&#34;\n    Return a new RDD by applying a function to each element of this RDD.\n\n    &gt;&gt;&gt; rdd = sc.parallelize([&#34;b&#34;, &#34;a&#34;, &#34;c&#34;])\n    &gt;&gt;&gt; sorted(rdd.map(lambda x: (x, 1)).collect())\n    [(&#39;a&#39;, 1), (&#39;b&#39;, 1), (&#39;c&#39;, 1)]\n    &#34;&#34;&#34;</span>\n    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">func</span><span class=\"params\">(_, iterator)</span>:</span>\n        <span class=\"keyword\">return</span> imap(f, iterator)\n    <span class=\"keyword\">return</span> self.mapPartitionsWithIndex(func, preservesPartitioning)\n\n<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mapPartitionsWithIndex</span><span class=\"params\">(self, f, preservesPartitioning=False)</span>:</span>\n    <span class=\"string\">&#34;&#34;&#34;\n    Return a new RDD by applying a function to each partition of this RDD,\n    while tracking the index of the original partition.\n\n    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4], 4)\n    &gt;&gt;&gt; def f(splitIndex, iterator): yield splitIndex\n    &gt;&gt;&gt; rdd.mapPartitionsWithIndex(f).sum()\n    6\n    &#34;&#34;&#34;</span>\n    <span class=\"keyword\">return</span> PipelinedRDD(self, f, preservesPartitioning)\n</code></pre><p>对于map filter这类函数来说，他们每次操作都是产生一个叫做PipelinedRDD的对象，那这个PipelinedRDD又是干什么的呢？</p>\n<pre><code><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">PipelinedRDD</span><span class=\"params\">(RDD)</span>:</span>\n\n    <span class=\"string\">&#34;&#34;&#34;\n    Pipelined maps:\n\n    &gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 4])\n    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()\n    [4, 8, 12, 16]\n    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).map(lambda x: 2 * x).collect()\n    [4, 8, 12, 16]\n\n    Pipelined reduces:\n    &gt;&gt;&gt; from operator import add\n    &gt;&gt;&gt; rdd.map(lambda x: 2 * x).reduce(add)\n    20\n    &gt;&gt;&gt; rdd.flatMap(lambda x: [x, x]).reduce(add)\n    20\n    &#34;&#34;&#34;</span>\n\n    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self, prev, func, preservesPartitioning=False)</span>:</span>\n        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> isinstance(prev, PipelinedRDD) <span class=\"keyword\">or</span> <span class=\"keyword\">not</span> prev._is_pipelinable():\n            \n            self.func = func\n            self.preservesPartitioning = preservesPartitioning\n            self._prev_jrdd = prev._jrdd\n            self._prev_jrdd_deserializer = prev._jrdd_deserializer\n        <span class=\"keyword\">else</span>:\n            prev_func = prev.func\n\n            <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">pipeline_func</span><span class=\"params\">(split, iterator)</span>:</span>\n                <span class=\"keyword\">return</span> func(split, prev_func(split, iterator))\n            self.func = pipeline_func\n            self.preservesPartitioning = \n                prev.preservesPartitioning <span class=\"keyword\">and</span> preservesPartitioning\n            self._prev_jrdd = prev._prev_jrdd  <span class=\"comment\"># maintain the pipeline</span>\n            self._prev_jrdd_deserializer = prev._prev_jrdd_deserializer\n        self.is_cached = <span class=\"keyword\">False</span>\n        self.is_checkpointed = <span class=\"keyword\">False</span>\n        self.ctx = prev.ctx\n        self.prev = prev\n        self._jrdd_val = <span class=\"keyword\">None</span>\n        self._id = <span class=\"keyword\">None</span>\n        self._jrdd_deserializer = self.ctx.serializer\n        self._bypass_serializer = <span class=\"keyword\">False</span>\n        self.partitioner = prev.partitioner <span class=\"keyword\">if</span> self.preservesPartitioning <span class=\"keyword\">else</span> <span class=\"keyword\">None</span>\n        self._broadcast = <span class=\"keyword\">None</span>\n</code></pre><p>我们可以看到，PipelinedRDD只是记录下当前操作但不执行所以每做一次rdd操作，只是记录下了对应的映射关系，数据集还是在原始状态。只有当使用到了reduce这类函数时才会被执行计算。</p>\n<pre><code><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">mean</span><span class=\"params\">(self)</span>:</span>\n    <span class=\"string\">&#34;&#34;&#34;\n    Compute the mean of this RDD&#39;s elements.\n\n    &gt;&gt;&gt; sc.parallelize([1, 2, 3]).mean()\n    2.0\n    &#34;&#34;&#34;</span>\n    <span class=\"keyword\">return</span> self.stats().mean()\n\n<span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">stats</span><span class=\"params\">(self)</span>:</span>\n    <span class=\"string\">&#34;&#34;&#34;\n    Return a L&#123;StatCounter&#125; object that captures the mean, variance\n    and count of the RDD&#39;s elements in one operation.\n    &#34;&#34;&#34;</span>\n    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">redFunc</span><span class=\"params\">(left_counter, right_counter)</span>:</span>\n        <span class=\"keyword\">return</span> left_counter.mergeStats(right_counter)\n\n    <span class=\"keyword\">return</span> self.mapPartitions(<span class=\"keyword\">lambda</span> i: [StatCounter(i)]).reduce(redFunc)\n</code></pre><p>这里，也是回到了PipelinedRDD，但是这次就不只保存待执行的函数了，而是通过jrdd执行</p>\n<pre><code>@property\ndef _jrdd(self):\n    <span class=\"keyword\">if</span> self._jrdd_val:\n        return self._jrdd_val\n    <span class=\"keyword\">if</span> self._bypass_serializer:\n        self._jrdd_deserializer = <span class=\"function\"><span class=\"title\">NoOpSerializer</span><span class=\"params\">()</span></span>\n\n    <span class=\"keyword\">if</span> self<span class=\"class\">.ctx</span><span class=\"class\">.profiler_collector</span>:\n        profiler = self<span class=\"class\">.ctx</span><span class=\"class\">.profiler_collector</span><span class=\"class\">.new_profiler</span>(self.ctx)\n    <span class=\"keyword\">else</span>:\n        profiler = None\n\n    command = (self<span class=\"class\">.func</span>, profiler, self._prev_jrdd_deserializer,\n               self._jrdd_deserializer)\n    pickled_cmd, bvars, env, includes = _prepare_for_python_RDD(self<span class=\"class\">.ctx</span>, command, self)\n    python_rdd = self<span class=\"class\">.ctx</span>._jvm.<span class=\"function\"><span class=\"title\">PythonRDD</span><span class=\"params\">(self._prev_jrdd.rdd()</span></span>,\n                                         <span class=\"function\"><span class=\"title\">bytearray</span><span class=\"params\">(pickled_cmd)</span></span>,\n                                         env, includes, self<span class=\"class\">.preservesPartitioning</span>,\n                                         self<span class=\"class\">.ctx</span><span class=\"class\">.pythonExec</span>,\n                                         bvars, self<span class=\"class\">.ctx</span>._javaAccumulator)\n    self._jrdd_val = python_rdd.<span class=\"function\"><span class=\"title\">asJavaRDD</span><span class=\"params\">()</span></span>\n\n    <span class=\"keyword\">if</span> profiler:\n        self._id = self._jrdd_val.<span class=\"function\"><span class=\"title\">id</span><span class=\"params\">()</span></span>\n        self<span class=\"class\">.ctx</span><span class=\"class\">.profiler_collector</span><span class=\"class\">.add_profiler</span>(self._id, profiler)\n    return self._jrdd_val\n</code></pre>","text":"一直以来写代码不求甚解，感觉这样不好，从今天开始起读各数据框架的源代码，学习学习再学习 今天看的是pyspark里lazy evaluation的处理，python和scala不同不是函数式的。那这是怎么办到的呢？ 首先所有的数据集在spark内部都叫做rdd，这在pyspark...","link":"","photos":[],"count_time":{"symbolsCount":"5.2k","symbolsTime":"5 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"toc":"","author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}},"mapped":true,"prev_post":{"title":"nginx lua waf","uid":"b7629b20ac900d8f1fafabf105306db6","slug":"zl/2016-01-01-125_nginx lua waf","date":"2024-04-03T03:47:33.031Z","updated":"2024-04-03T03:47:33.031Z","comments":true,"path":"api/articles/zl/2016-01-01-125_nginx lua waf.json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":"ngx_lua_waf是一个基于lua-nginx-module(openresty)的web应用防火墙###用途： 防止sql注入，本地包含，部分溢出，fuzzing测试，xss,SSRF等web攻击 防止svn/备份之类文件泄漏 防止ApacheBench之类压力测试工具的攻...","link":"","photos":[],"count_time":{"symbolsCount":"2.1k","symbolsTime":"2 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"Unity中使用sLua的 超丶简单基础教程(一)","uid":"31f7798c17cec129652145a050a37985","slug":"zl/2016-01-01-127_Unity中使用sLua的 超丶简单基础教程(一)","date":"2024-04-03T03:47:33.031Z","updated":"2024-04-03T03:47:33.031Z","comments":true,"path":"api/articles/zl/2016-01-01-127_Unity中使用sLua的 超丶简单基础教程(一).json","keywords":"AIGC,LLM,糖果AIGC实验室","cover":"https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg","text":" 前言网上Unity使用lua的文章本来就少..slua又是个偏小众的lua库..文章更少.. 已有的文章又有点坑…比如方法名关键字写错啦..真的是坑多= = 所以总结一片超简单教学= = 这篇文章面向刚开始学习lua 想在unity项目中使用sLua库的同学.. 正文导入sLu...","link":"","photos":[],"count_time":{"symbolsCount":"1.4k","symbolsTime":"1 mins."},"categories":[{"name":"topic","slug":"topic","count":1441,"path":"api/categories/topic.json"}],"tags":[{"name":"lua文章","slug":"lua文章","count":1133,"path":"api/tags/lua文章.json"}],"author":{"name":"安全书","slug":"blog-author","avatar":"https://img-blog.csdnimg.cn/20210313122054101.png","link":"/","description":"《墨守之道-Web服务安全架构与实践》","socials":{"github":"https://github.com/shengnoah","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"https://weibo.com/u/3732639263","zhihu":"https://www.zhihu.com/people/openresty","csdn":"","juejin":"","customs":{}}}}}